{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "WqDB-xPi4hId"
      },
      "outputs": [],
      "source": [
        "  # import libraries\n",
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from glob import glob\n",
        "from keras.layers import Dense, Dropout, Flatten,Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, Conv3D, BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img,img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram_equalization(image):\n",
        "    # Chuyển đổi ảnh sang ảnh xám\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Tính toán histogram của ảnh xám\n",
        "    hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
        "    \n",
        "    # Tính toán histogram tích lũy\n",
        "    cum_hist = hist.cumsum()\n",
        "    \n",
        "    # Chuẩn hóa histogram tích lũy\n",
        "    cum_hist_normalized = cum_hist / cum_hist.max()\n",
        "    \n",
        "    # Ánh xạ lại các giá trị cường độ từ histogram tích lũy chuẩn hóa\n",
        "    mapping = (cum_hist_normalized * 255).astype(np.uint8)\n",
        "    \n",
        "    # Áp dụng phép ánh xạ lại lên ảnh gốc\n",
        "    equalize_image = mapping[gray_image.astype(np.uint8)]\n",
        "    \n",
        "    return equalize_image\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Đường dẫn đến thư mục chứa dataset trên Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/MV/MV FINAL/TRAIN'\n",
        "\n",
        "# Danh sách nhãn\n",
        "labels = ['COVID19', 'HEALTHY', 'PNEUMONIA']\n",
        "\n",
        "processed_images = []\n",
        "processed_labels = []\n",
        "\n",
        "# Duyệt qua từng nhãn\n",
        "for label in labels:\n",
        "    label_path = dataset_path + '/' + label  # Đường dẫn đến thư mục của nhãn hiện tại\n",
        "    \n",
        "    # Duyệt qua từng ảnh trong nhãn\n",
        "    for image_name in os.listdir(label_path):\n",
        "        image_path = label_path + '/' + image_name  # Đường dẫn đến ảnh hiện tại\n",
        "\n",
        "        # Đọc ảnh và áp dụng histogram equalization\n",
        "        image = cv2.imread(image_path)\n",
        "        equalized_image = histogram_equalization(image)\n",
        "\n",
        "        processed_images.append(equalized_image)\n",
        "        processed_labels.append(label)\n",
        "\n",
        "# Chuyển đổi danh sách ảnh và nhãn thành mảng numpy\n",
        "processed_images = np.array(processed_images)\n",
        "processed_labels = np.array(processed_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lWm8NDy4sbd",
        "outputId": "e7612618-5243-4c40-ce70-cad6e68a9980"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-d0d54aa2b188>:50: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  processed_images = np.array(processed_images)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra kích thước bộ dataset mới\n",
        "print(\"Kích thước bộ dataset mới:\", processed_images.shape)\n",
        "print(\"Số nhãn trong bộ dataset mới:\", len(np.unique(processed_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVuUJ7-v6UAd",
        "outputId": "1fc69b79-f6f2-43ff-9ae9-33ca3c53fae4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kích thước bộ dataset mới: (5478,)\n",
            "Số nhãn trong bộ dataset mới: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_indices = {}\n",
        "\n",
        "for idx, label in enumerate(labels):\n",
        "    label_indices[label] = np.where(processed_labels == label)[0]\n",
        "\n",
        "# In thông tin về từ điển\n",
        "for label, indices in label_indices.items():\n",
        "    print(f\"{label}: {len(indices)} ảnh\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yui8n60Y6Wzt",
        "outputId": "a7c99666-4a02-4fd3-db06-d7ddd982df06"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COVID19: 140 ảnh\n",
            "HEALTHY: 1815 ảnh\n",
            "PNEUMONIA: 3523 ảnh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chia tập dữ liệu thành tập huấn luyện và tập kiểm tra\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_images, processed_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "from skimage.transform import resize\n",
        "\n",
        "#Resize ảnh về cùng 1 kích thước\n",
        "resized_images_train = []\n",
        "for image in X_train:\n",
        "    resized_image = resize(image, (150, 150, 3))\n",
        "    resized_images_train.append(resized_image)\n",
        "\n",
        "X_train = np.array(resized_images_train)\n",
        "\n",
        "resized_images_test = []\n",
        "for image in X_test:\n",
        "    resized_image = resize(image, (150, 150, 3))\n",
        "    resized_images_test.append(resized_image)\n",
        "\n",
        "X_test = np.array(resized_images_test)\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "label_mapping = {'COVID19': 0, 'HEALTHY': 1, 'PNEUMONIA': 2}\n",
        "\n",
        "# Chuyển đổi nhãn thành dạng số nguyên\n",
        "y_train = np.array([label_mapping[label] for label in y_train])\n",
        "y_test = np.array([label_mapping[label] for label in y_test])\n",
        "\n",
        "# Chuyển đổi nhãn thành one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train)\n",
        "X_test = tf.convert_to_tensor(X_test)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "y_test = tf.convert_to_tensor(y_test)\n"
      ],
      "metadata": {
        "id": "pb8c4Szc5Dj_"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define và compile CNN model\n",
        "model=Sequential()\n",
        "\n",
        "# Nhân tích chập 32 lần\n",
        "model.add(Conv2D(32,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same',input_shape=(150,150,3))) \n",
        "#Hàm loại bỏ\n",
        "model.add(LeakyReLU(alpha = 0.1))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "# Nhân tích chập 64 lần \n",
        "model.add(Conv2D(64,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same')) \n",
        "#Hàm loại bỏ\n",
        "model.add(LeakyReLU(alpha = 0.1))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "# Nhân tích chập 128 lần \n",
        "model.add(Conv2D(128,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same')) \n",
        "#Hàm loại bỏ\n",
        "model.add(LeakyReLU(alpha = 0.1))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256,activation='relu',kernel_initializer='he_uniform'))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJcM8gTk5Jv3",
        "outputId": "0d7693ac-8071-422f-8d00-a67b90ccd1f9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 150, 150, 32)      896       \n",
            "                                                                 \n",
            " leaky_re_lu_18 (LeakyReLU)  (None, 150, 150, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  (None, 75, 75, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 75, 75, 64)        18496     \n",
            "                                                                 \n",
            " leaky_re_lu_19 (LeakyReLU)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 37, 37, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 37, 37, 128)       73856     \n",
            "                                                                 \n",
            " leaky_re_lu_20 (LeakyReLU)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPoolin  (None, 18, 18, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 41472)             0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               10617088  \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,711,107\n",
            "Trainable params: 10,711,107\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Biên dịch mô hình\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "#Lưu lại mô hình\n",
        "model.save(\"/content/drive/MyDrive/MV/MV FINAL/mv_final.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUv_Pmtf5ObC",
        "outputId": "8f0f7a24-65ac-49f9-d6e2-fc80950e4dd1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "137/137 [==============================] - 292s 2s/step - loss: 1.2839 - accuracy: 0.6853 - val_loss: 0.6092 - val_accuracy: 0.7290\n",
            "Epoch 2/20\n",
            "137/137 [==============================] - 343s 3s/step - loss: 0.5605 - accuracy: 0.7494 - val_loss: 0.5621 - val_accuracy: 0.7336\n",
            "Epoch 3/20\n",
            "137/137 [==============================] - 299s 2s/step - loss: 0.5158 - accuracy: 0.7741 - val_loss: 0.5416 - val_accuracy: 0.7655\n",
            "Epoch 4/20\n",
            "137/137 [==============================] - 295s 2s/step - loss: 0.4787 - accuracy: 0.7793 - val_loss: 0.5629 - val_accuracy: 0.7372\n",
            "Epoch 5/20\n",
            "137/137 [==============================] - 290s 2s/step - loss: 0.4321 - accuracy: 0.8085 - val_loss: 0.5748 - val_accuracy: 0.7336\n",
            "Epoch 6/20\n",
            "137/137 [==============================] - 286s 2s/step - loss: 0.3836 - accuracy: 0.8241 - val_loss: 0.5908 - val_accuracy: 0.7345\n",
            "Epoch 7/20\n",
            "137/137 [==============================] - 288s 2s/step - loss: 0.3365 - accuracy: 0.8514 - val_loss: 0.6481 - val_accuracy: 0.7062\n",
            "Epoch 8/20\n",
            "137/137 [==============================] - 284s 2s/step - loss: 0.2802 - accuracy: 0.8795 - val_loss: 0.7731 - val_accuracy: 0.7062\n",
            "Epoch 9/20\n",
            "137/137 [==============================] - 285s 2s/step - loss: 0.2463 - accuracy: 0.8934 - val_loss: 0.6841 - val_accuracy: 0.7327\n",
            "Epoch 10/20\n",
            "137/137 [==============================] - 286s 2s/step - loss: 0.2004 - accuracy: 0.9142 - val_loss: 0.7866 - val_accuracy: 0.7181\n",
            "Epoch 11/20\n",
            "137/137 [==============================] - 286s 2s/step - loss: 0.1535 - accuracy: 0.9370 - val_loss: 0.9363 - val_accuracy: 0.7308\n",
            "Epoch 12/20\n",
            "137/137 [==============================] - 302s 2s/step - loss: 0.1245 - accuracy: 0.9505 - val_loss: 0.8916 - val_accuracy: 0.7308\n",
            "Epoch 13/20\n",
            "137/137 [==============================] - 288s 2s/step - loss: 0.1031 - accuracy: 0.9617 - val_loss: 1.1061 - val_accuracy: 0.7162\n",
            "Epoch 14/20\n",
            "137/137 [==============================] - 291s 2s/step - loss: 0.0799 - accuracy: 0.9726 - val_loss: 1.1532 - val_accuracy: 0.7208\n",
            "Epoch 15/20\n",
            "137/137 [==============================] - 315s 2s/step - loss: 0.0745 - accuracy: 0.9717 - val_loss: 1.2287 - val_accuracy: 0.7144\n",
            "Epoch 16/20\n",
            "137/137 [==============================] - 287s 2s/step - loss: 0.0409 - accuracy: 0.9879 - val_loss: 1.5694 - val_accuracy: 0.7199\n",
            "Epoch 17/20\n",
            "137/137 [==============================] - 287s 2s/step - loss: 0.0541 - accuracy: 0.9808 - val_loss: 1.5001 - val_accuracy: 0.7345\n",
            "Epoch 18/20\n",
            "137/137 [==============================] - 288s 2s/step - loss: 0.0220 - accuracy: 0.9925 - val_loss: 1.7355 - val_accuracy: 0.7026\n",
            "Epoch 19/20\n",
            "137/137 [==============================] - 305s 2s/step - loss: 0.0615 - accuracy: 0.9801 - val_loss: 1.4943 - val_accuracy: 0.7108\n",
            "Epoch 20/20\n",
            "137/137 [==============================] - 290s 2s/step - loss: 0.0300 - accuracy: 0.9929 - val_loss: 1.7924 - val_accuracy: 0.7308\n"
          ]
        }
      ]
    }
  ]
}